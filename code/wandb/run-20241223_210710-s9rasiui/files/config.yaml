_wandb:
    value:
        cli_version: 0.19.1
        m: []
        python_version: 3.8.0
        t:
            "1":
                - 1
                - 11
                - 49
                - 51
                - 55
                - 71
                - 84
                - 98
            "2":
                - 1
                - 11
                - 49
                - 51
                - 55
                - 71
                - 84
                - 98
            "3":
                - 13
                - 16
                - 23
                - 55
            "4": 3.8.0
            "5": 0.19.1
            "6": 4.46.3
            "8":
                - 5
            "12": 0.19.1
            "13": linux-x86_64
dataset_dir:
    value: ../../data/
gradient_accumulation_steps:
    value: 8
learning_rate:
    value: 2e-06
lora_alpha:
    value: 32
lora_dropout:
    value: 0.01
lora_r:
    value: 128
lr_scheduler_type:
    value: cosine
max_grad_norm:
    value: 0.3
max_seq_length:
    value: 1024
mode:
    value: train
model_id:
    value: google/gemma-2-9b
num_train_epochs:
    value: 3
output_dir:
    value: /media/hdd/llm4math/
per_device_train_batch_size:
    value: 1
project_name:
    value: gemma_sft_project_lora_mistral
quantization_bits:
    value: 4
run_name:
    value: gemma_train_run_9b_A6000
warmup_steps:
    value: 1000
weight_decay:
    value: 0.01
